class CodeGenTemplate:

  @classmethod
  def autogen_head(cls):
    return '''# Autogenerated by onnx-pytorch.
'''

  @classmethod
  def imports(cls):
    return '''import glob
import os
import math

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

class KConv2d(nn.Conv2d):
    #docstring for KConv2d.

    def __init__(self, in_channels, out_channels, kernel_size,
                 stride=1, padding=0, dilation=1, groups=1, bias=True):
        zeropadding = (0,0)
        padding_ = padding
        super(KConv2d, self).__init__(in_channels, out_channels, kernel_size,
                                      stride, (0, 0), dilation, groups, bias)
        self.zeropad2d = nn.ZeroPad2d(padding)
        
    def _padded_forward(self, input):
        padinput = self.zeropad2d(input)
        bias = self.bias.data if self.bias else None
        output = F.conv2d(padinput, self.weight.data, bias, self.stride,
                        self.padding, self.dilation, self.groups)        
        return output
      
    def forward(self, input):
        return self._padded_forward(input)
        
'''

  @classmethod
  def model(cls, model_init, model_forward, model_method, test_run_model):
    return f'''{cls.autogen_head()}
{cls.imports()}

class Model(nn.Module):
  def __init__(self):
    super(Model, self).__init__()
    self._vars = nn.ParameterDict()
    self._regularizer_params = []
    for b in glob.glob(
        os.path.join(os.path.dirname(__file__), "variables", "*.npy")):
      v = torch.from_numpy(np.load(b))
      requires_grad = v.dtype.is_floating_point or v.dtype.is_complex
      self._vars[os.path.basename(b)[:-4]] = nn.Parameter(v, requires_grad=requires_grad)
    {model_init}

  def forward(self, *inputs):
    {model_forward}

  {model_method}
{test_run_model}
'''
